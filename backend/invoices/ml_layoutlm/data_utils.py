import json
import cv2 # Assuming you have opencv-python installed as instructed
import numpy as np # Import numpy
from transformers import LayoutLMv3Processor
import os
import torch

# Ensure the path to the sample_ocr_json.json is correct
# It was exported to backend/invoices/ml_layoutlm/
current_dir = os.path.dirname(os.path.abspath(__file__))
sample_json_path = os.path.join(current_dir, "invoice_69_ocr_json.json")

processor = LayoutLMv3Processor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)

def ocr_json_to_layoutlm_inputs(ocr_json, image_path=None):
    words, boxes = [], []
    image = None

    if image_path and os.path.exists(image_path):
        try:
            image = cv2.imread(image_path)
            if image is None:
                print(f"Warning: Could not read image from {image_path}. Proceeding with dummy image.")
        except Exception as e:
            print(f"Error loading image {image_path}: {e}. Proceeding with dummy image.")
    
    # If no image is loaded or provided, create a dummy blank image to satisfy LayoutLMv3Processor
    if image is None:
        # Create a blank white image (e.g., 1000x1000 pixels, 3 channels, uint8 type)
        image = np.ones((1000, 1000, 3), dtype=np.uint8) * 255
        print("Using a dummy blank image as input to LayoutLMv3Processor.")

    # Modify this loop to match the structure generated by generate_ocr_jsons_from_images.py
    for page in ocr_json.get("pages", []):
        # For simplicity, we assume one page and the bbox is already normalized
        # If the generated JSON is simpler, we directly access words and bboxes
        for block in page.get("blocks", []):
            for line in block.get("lines", []):
                for word_entry in line.get("words", []):
                    txt = word_entry.get("text", "")
                    bbox = word_entry.get("bbox", [0,0,0,0]) # Bbox is already normalized 0-1000

                    if not txt.strip():
                        continue

                    # Ensure coordinates are valid and within [0, 1000]
                    # The bbox from generate_ocr_jsons_from_images.py is already normalized.
                    # We just need to ensure it's a valid box before appending.
                    x0, y0, x1, y1 = bbox
                    if not (0 <= x0 <= 1000 and 0 <= y0 <= 1000 and 0 <= x1 <= 1000 and 0 <= y1 <= 1000 and x1 >= x0 and y1 >= y0):
                        print(f"Warning: Skipping invalid normalized box for word: {txt} - {bbox}")
                        continue

                    boxes.append(bbox)
                    words.append(txt)

    if not words or not boxes:
        print("Warning: No valid words or boxes extracted from OCR JSON. Returning empty encoding.")
        # Return empty tensors for input_ids, attention_mask, bbox, and pixel_values
        return {
            "input_ids": torch.tensor([]).long(),
            "attention_mask": torch.tensor([]).long(),
            "bbox": torch.tensor([[]]).long(),
            "pixel_values": torch.tensor([[[[]]]]).float(), # Dummy for a 4D tensor if image is not present
            "labels": torch.tensor([]).long() # Add labels as well, even if empty, for consistency
        }

    # LayoutLMv3Processor will tokenize & align these
    encoding = processor(
        image,         # Now guaranteed to be a valid image (real or dummy)
        words,
        boxes=boxes,
        return_tensors="pt",
        padding="max_length", # Pad to max_length
        truncation=True,      # Truncate if longer than max_length
        max_length=512,       # Set max sequence length for LayoutLMv3
        return_offsets_mapping=True  # Used to align tokens to words for labeling
    )
    return encoding

if __name__ == "__main__":
    # quick sanity check
    # Load the exported sample OCR JSON
    try:
        with open(sample_json_path, "r", encoding="utf-8") as f:
            sample_ocr_json = json.load(f)
    except FileNotFoundError:
        print(f"Error: {sample_json_path} not found. Please ensure export_ocr_json.py was run successfully and the path is correct.")
        exit()
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from {sample_json_path}. Invalid JSON format.")
        exit()

    # The original image file path would be invoice.file.path. 
    # For this sanity check, we can omit it or try to infer a typical path
    # For instance, if invoice 69 image is stored as: 
    # backend/media/invoices/invoice_template_2_DRhkkqW.jpg
    # image_file_path = os.path.join(os.path.dirname(current_dir), "media", "invoices", "invoice_template_2_DRhkkqW.jpg")
    # For now, let's run text-only as per ChatGPT's suggestion to start.
    
    enc = ocr_json_to_layoutlm_inputs(sample_ocr_json, image_path=None)
    
    if enc:
        print(f"Encoding keys: {enc.keys()}")
        print(f"Input IDs shape: {enc['input_ids'].shape}")
        print(f"BBox shape: {enc['bbox'].shape}")
        
        # Inspect a few token-to-box mappings
        print("\nFirst 5 tokens and their boxes:")
        for i in range(min(5, enc["input_ids"].shape[1])):
            token_id = enc["input_ids"][0, i].item()
            bbox = enc["bbox"][0, i].tolist()
            # Convert token ID back to string (approximate, for visualization)
            word = processor.tokenizer.decode([token_id])
            print(f"  Token: '{word}', Box: {bbox}")

        print("\nLast 5 tokens and their boxes:")
        for i in range(max(0, enc["input_ids"].shape[1] - 5), enc["input_ids"].shape[1]):
            token_id = enc["input_ids"][0, i].item()
            bbox = enc["bbox"][0, i].tolist()
            word = processor.tokenizer.decode([token_id])
            print(f"  Token: '{word}', Box: {bbox}")
    else:
        print("Failed to generate LayoutLM inputs.") 